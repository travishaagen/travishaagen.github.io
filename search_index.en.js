window.searchIndex = [{"url":"https://travishaagen.github.io/","title":"Home","body":"About\nHey, I'm Travis Haagen. I've been a software engineer for a while.\nReach me on LinkedIn or BlueSky.\n"},{"url":"https://travishaagen.github.io/posts/","title":"Posts","body":""},{"url":"https://travishaagen.github.io/posts/backpressure-in-client-server-applications/","title":"Backpressure in Client-Server Applications","body":"Backpressure, in client-server applications, is accomplished when a client adjusts its transmission of\nmessages in response to a server that has slowed its processing of messages.\nStuck in Traffic\nIdeally, when a networked client connects to a server, messages are sent at maximum speed and responses are likewise\nreturned quickly. To handle more client traffic we scale horizontally by adding additional server instances and\nour long-tail latency metrics are stable over time. However, in most real world applications the server is not a\nmonolith unto itself and will make internal requests to relational databases and external APIs with unpredictable\nperformance characteristics.\nTimeouts are essential for achieving resiliency. When a timeout is reached, the client disconnects and potentially\ntries again until it gives up. The absence of sensible timeouts results in resource consumption (e.g., network sockets,\nmemory, threads) and conditions resembling a traffic jam. In a healthy system both the client and server will monitor\nconnections and proactively timeout.\nConstantly connecting and disconnecting, in the face of server instability, has a cost. On average, it takes longer to\nestablish a network connection, and negotiate a secure socket, than to issue requests and responses. An\nalternative is for the client to respond to backpressure from the server by keeping the connection open and only\nsending data when the server is ready to accept it. Instead of a chaotic traffic jam, the stream of messages is like a\ncargo train, with slow-downs anticipated.\nBackpressure\nThe Transmission Control Protocol (TCP) considers backpressure as part of the specification.\n“Understanding TCP Protocol and Backpressure\n”¹ does a nice job summarizing the key ideas.\n\n\nTCP implements flow control to ensure that the sender does not overwhelm the receiver with data. It uses a sliding\nwindow mechanism to control the number of unacknowledged segments that can be sent at a time.\nThe receiver advertises its window size, indicating the amount of data it can currently accept.\nThe sender adjusts the rate of transmission based on the receiver’s window size, ensuring efficient data transfer.\n\n\nIn practice, for backpressure to work effectively, both the client and the server should adhere to some specific design\nprinciples:\n\nClient &amp; Server\n\nConnection should be long-lived (e.g., HTTP keep-alive, websocket, etc.)\n\n\nClient\n\nNetworking libraries should indicate when the socket is writable\nTimeouts should be tuned to allow for backpressure\n\n\nServer\n\nSeparate thread-pools should handle,\n\nAccepting network connections\nSending and receiving data over established connections\n\n\nSlow or idle clients should be detected and pruned\n\n\n\nLet us consider an asynchronous, event-driven application architecture. On the server a single thread can effectively\nhandle the job of accepting network connections. Once connected, another thread, or pool of threads, can read requests\nand send responses. These \"worker threads\" must take care not block on I/O or mutexes. We read the request data, make\nasynchronous calls to internal services, and during those asynchronous calls our worker threads may service other\nrequests. Once our asynchronous work is complete, and if the client is still connected, we write a response. This\ncould be backpressure at work, but how can we know for certain? Unfortunately, most of us use software frameworks\nthat make it difficult to know what is going on beneath the surface.\nAn Experiment\nFor this article we created a project at\nhttps://github.com/travishaagen/blog-backpressure. It's only\ndependency is Netty, which is an event-driven abstraction over Java's internal\nsockets and byte buffers. To prove to ourselves that backpressure is occurring, the application establishes a\nsingle websocket connection. The client has a Netty event-loop with a single thread.\nLikewise, the server has a single-threaded connection acceptor and a single threaded worker-pool, which attempts to\nwrite to a bounded queue of size 1. A consumer thread reads from the bounded queue at a slow rate. When the queue\nis full, we cannot write to it, so we stop reading data. When we stop reading, the client is informed that\nthe server cannot accept more data, which causes the client to stop writing. This is backpressure.\n\n\n\n\n\n\nEvery network connection establishes a\nChannel. A Channel\nhas a method called isWritable() with the following documentation,\n\nReturns true if and only if the I/O thread will perform the requested write operation immediately. Any write\nrequests made when this method returns false are queued until the I/O thread is ready to process the queued write\nrequests.\nWriteBufferWaterMark\ncan be used to configure on which condition the write buffer would cause this channel to change writability.\n\nIn our WebSocketClient you can see that, in a loop, we increment a 64-bit integer and only write it to the channel\nwhen writable. The client is detecting and adapting to a full write buffer.\n\nOn the server, to control our experiment, we create a bounded queue of size 1.\n\nWe then pass the queue to a \"consumer thread\" which reads from it at a slow pace, to simulate a backlog\nof work.\n\nWe also pass the queue to our WebSocketFrameHandler. It reads from the Channel and will spin in a loop\nuntil it can write to the queue. It does not perform any additional reads until the data is written, which causes\nbackpressure.\n\nWhen we run the application we'll see an output similar to the following. The Wrote lines show the number of messages\nwritten to the socket and Consumed shows the count of messages read by the server's \"consumer thread\".\nNote that, to save space, only the last consecutive Wrote/Consumed lines are shown below.\n\nWhen we observe the loopback interface in Wireshark and set the filter to\ntcp.window_size == 0 we see that the TCP window drops to zero multiple times. This is what causes the client to stop\nwriting.\n\n\n\n\n\n\nFinal Thoughts\nBackpressure is an elegant concept in computer networking. As the above experiment shows, both the client and\nserver need to actively control reading and writing socket data for it to work properly. Websockets seem particularly\nwell suited to applications that seek to control backpressure, because it's easy to conceive of inbound messages\nfeeding into one queue and outbound messages into another. We hope that this discussion will assist developers in\nthinking about backpressure and how well their libraries and frameworks support it.\nFor further reading we suggest\n“Applying Back Pressure When Overloaded\n”² by Martin Thompson.\nReferences\n\nKumar, A. (2023, August 6). Understanding TCP Protocol and Backpressure. Sum Of Bytes.\nhttps://sumofbytes.com/blog/understanding-tcp-protocol-and-backpressure/\nThompson, M. (2012, May 19). Applying Back Pressure When Overloaded. Mechanical Sympathy.\nhttps://mechanical-sympathy.blogspot.com/2012/05/apply-back-pressure-when-overloaded.html\n\n"},{"url":"https://travishaagen.github.io/posts/static-exceptions-for-flow-control/","title":"Static Exceptions for Flow Control","body":"Static Exceptions, when used for flow control, can dramatically reduce garbage generation and latency in Java and Kotlin\napplications.\nTL;DR\nTo create high performance exceptions for flow control in the JVM, simply extend the following StaticException class.\nThen store the Exception instance in a static final field and throw it as needed. They're fast, because the call\nstack doesn't need to be traversed and since we're creating reusable objects, there's less for the garbage collector\nto do.\nJava:\n\nKotlin:\n\nExample Use Case\nIn a web application, conditions that trigger HTTP error responses can happen anywhere in the call chain. For example,\nthe client's bearer token has expired, so we return 401 Unauthorized. A request is malformed, so validation\nlogic returns 400 Bad Request. Likewise, deeper in the call chain, an asynchronous API call times out, so we return\n503 Service Unavailable. None of these error conditions are particularly unexpected, and aside from the timeout,\nmay not even warrant logging.\nFor JVM applications it can be convenient to throw an exception that signifies \"respond with an error\". When the\nexception is caught, we build the appropriate HTTP response. An example response exception might be,\n\nWhich could be caught in our web controller logic, or a framework mechanism such as a Spring\n@ExceptionHandler,\nwhere the appropriate HTTP response status header and payload body would be constructed,\n\nDiscussion\nWe're using exceptions for flow control. Unfortunately, from a performance perspective, exceptions are expensive.\nThe bulk of the cost comes from Throwable.fillInStackTrace() which traverses the\ncall stack, and is exacerbated by massively long call stacks created by\nlibraries and frameworks that we use to make life easier.\nFortunately, there is a solution. We can preinitialize exceptions and either generate the call stack once, or\nprovide an empty stack. I first learned about this technique from Norman Maurer's blog post\nentitled “The hidden performance costs of instantiating Throwables”¹.\nNorman is one of the creators of Netty which is an abstraction over Java's internal\nsockets and byte buffers. It's a technique that has been used sparingly in Netty itself.\nHowever, if you do it wrong, it can lead to problems. Let's look at this constructor for Throwable,\n\nIt's most essential that we set enableSuppression to false. Otherwise, our own code or magical side effects from\nlibraries could invoke the addSuppressed method, and potentially add objects to the suppressedExceptions\ncollection every time we throw the reusable exception. The collection would grow infinitely.\n\nThis happened to me (:faceplant:). It turns out that Project Reactor invokes\naddSuppressed in a class called\nFluxOnAssembly\nevery time an Exception passes through.\nBenchmarks\nNorman included some Java Microbenchmark Harness (JMH) results in the\nbeforementioned blog post.\nI also ran across\n“Why Consuming Stack Traces is Noticeably Slower in Java 11 Compared to Java 8: JMH Benchmark Results.”²\nIt makes some interesting points and I didn't realize that Java 11 introduced a change that was meant to improve\naverage performance by lazily traversing the call stack for logging. Because of this lazy traversal, I included\nsome additional *AndGetStacktrace JMH benchmarks below.\n\nTo summarize,\n\nThrowing and catching a reusable StaticException reached 1,799,330,563 operations/second\nReusable StaticException followed by a call to e.getStackTrace() was 105,525,289 operations/second\nThrowing a new RuntimeException every time was 1,529,637 operations/second\nAnd RuntimeException with e.getStackTrace() was 328,081 operations/second\n\nComparing the flow control static use case #1 with new #3 shows 1000x better performance for\nStaticException.\nBoth the static and new cases are seriously hampered whenever you choose to log the stacktrace.\nThe result for staticExceptionAndGetStacktrace is surprising, because the only additional operation is calling\nclone()\non our empty StackTraceElement[] array. Long call stacks were not simulated with the benchmarks either.\nAbove benchmark code available at\nhttps://github.com/travishaagen/blog-static-exceptions-for-flow-control\nFinal Thoughts\nThrowing exceptions has a cost in the JVM. For optimal performance, exceptions should be thrown sparingly.\nWhen a failure condition is not unexpected, your application's latency and throughput can benefit\nfrom static exceptions.\nReferences\n\nMaurer, N. (2013, November 9). The hidden performance costs of instantiating Throwables. The Thoughts of Norman\nMaurer. http://normanmaurer.me/blog/2013/11/09/The-hidden-performance-costs-of-instantiating-Throwables/\nWhy Consuming Stack Traces is Noticeably Slower in Java 11 Compared to Java 8: JMH Benchmark Results.\n(2025, November 26). javaspring.net.\nhttps://www.javaspring.net/blog/consuming-stack-traces-noticeably-slower-in-java-11-than-java-8/\n\n"}]